
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Human View Synthesis using a Single Sparse RGB-D Input</title>

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Human View Synthesis using a Single Sparse RGB-D Input 
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://phongnhhn.info/">
                          Phong Nguyen 
                        </a> <sup>1, 2</sup>
                        <!-- </br>University of Oulu -->
                    </li>
                    <li>
                        <a href="https://nsarafianos.github.io/">
                            Nikolaos Sarafianos 
                        </a> <sup>2</sup>
                        <!-- </br>Meta -->
                    </li>
                    <li>                
                        <a href="https://christophlassner.de/">
                            Christoph Lassner 
                        </a> <sup>2</sup>
                        <!-- </br>Meta -->
                    </li><br>
                    <li>
                        <a href="https://www.oulu.fi/university/researcher/janne-heikkila">
                            Janne Heikkilä 
                        </a> <sup>1</sup>
                        <!-- </br>University of Oulu -->
                    </li>
                    <li>
                        <a href="https://sites.google.com/site/tony2ng/">
                          Tony Tung 
                        </a> <sup>2</sup>
                        <!-- </br>Meta -->
                    </li>
                </ul>
            </div>
        </div>
        <div class="col-md-12 text-center">
            <sup>1</sup> University of Oulu &ensp; &ensp;<sup>2</sup> Meta Reality Labs Research, Sausalito
        </div>

        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="img/HVSNet.pdf">
                            <image src="img/img.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="img/HVSNet-supp.pdf">
                            <image src="img/img_supp.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="#">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <!-- <li>
                            <a href="#">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/teaser.mp4" type="video/mp4" />
                </video>
			</div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Novel view synthesis for humans in motion is a challenging computer vision problem that enables applications such as free-viewpoint video. Existing methods typically use complex setups with multiple input views, 3D supervision or pre-trained models that do not generalize well to new identities. Aiming to address these limitations, we present a novel view synthesis framework to generate realistic renders from unseen views of any human captured from a single-view sensor with sparse RGB-D, similar to a low-cost depth camera, and without actor-specific models. We propose an architecture to learn dense features in novel views obtained by sphere-based neural rendering, and create complete renders using a global context inpainting model.
                    Additionally, an enhancer network leverages the overall fidelity, even in occluded areas from the original view, producing crisp renders with fine details. We show our method generates high-quality novel views of synthetic and real human actors given a single sparse RGB-D input. It generalizes to unseen identities, new poses and faithfully reconstructs facial expressions. Our approach outperforms prior human view synthesis methods and is robust to different levels of input sparsity.
                </p>
            </div>
        </div>



        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="#" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Comparison of 3D point cloud transformations.
                </h3>
                <p class="text-justify">
                    From a single RGB-D input, we obtain the warped image using: a depth-based warping transformation, neural point and sphere-based renderer. The novel image warped by Pulsar is significantly denser because Pulsar renderer not only provides the option to use a per-sphere radius parameter, but it also provides gradients for these radiuses, which enables to set them dynamically. 
                </p>
                <p style="text-align:center;">
                    <image src="img/compare.png" height="10px" class="img-responsive">
                </p>                
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Sphere-based view synthesis network
                </h3>
                <p class="text-justify">
                    The feature predictor F learns radius and feature vectors of the sphere set S. We then use the sphere-based differentiable renderer Ω to densify the learned input features M and warp them to the target camera T . The projected features are passed through the global context inpainting module G to generate the foreground mask, confidence map and novel image. Brighter colors of the confidence map indicate lower confidence.
                </p>
                <p style="text-align:center;">
                    <image src="img/overview.png" height="5px" class="img-responsive">
                </p>                
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Enhancer network
                </h3>
                <p class="text-justify">
                    Using an additional occlusion-free input, we refine the initial estimated novel view by training the Enhancer network. We infer the dense correspondences of both predicted novel view and occlusion-free image using a novel HD-IUV module. The occlusion-free image is warped to the target view and then refined by an auto-encoder. The refined novel view shows better result on the occluded area compared to the initial estimated.
                </p>
                <p style="text-align:center;">
                    <image src="img/enhancer.png" height="10px" class="img-responsive">
                </p>                
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                <p class="text-justify">
                    We train our method on the synthetic data of the RenderPeople dataset and test the trained model on the real 3DMD scans. Qualitative results show that HVS-Net is able to generalize on unseen humans in the testing time. 
                </p>
                <p style="text-align:center;">
                    <image src="img/results.png" height="50px" class="img-responsive">
                </p>     
                <p class="text-justify">
                    Using a single sparse RGB-D input, point-based rendering method (SynSin) is unable to render realistic textures on the occluded regions at the target viewpoint and the skin of human is clearly different from the input image. Using just 10% of the input depth points from a single view, HVS-Net also outperform a recently proposed LookingGood method which utilizes a multi-view capture setup. 
                </p> 
                <video id="v0" width="100%" autoplay loop muted controls>
                    <source src="img/compare_vid.mp4" type="video/mp4" />
                </video>                    
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Input depth sparsity robustness
                </h3>
                <p class="text-justify">
                    Finally, we show the generated novel views of HVS-Net using different level of input depth sparsity. We observe a drop of performance of HVS-Net if we use only 5% of the points in the input depth map and there is no significant different between generated novel views using 10% or 25% of the input depth points 
                </p>
                <video id="v0" width="100%" autoplay loop muted controls>
                    <source src="img/depth_sparsity.mp4" type="video/mp4" />
                </video>                 
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{nguyen2021human,
    title={Human View Synthesis using a Single Sparse RGB-D Input}, 
    author={Phong Nguyen and Nikolaos Sarafianos and Christoph Lassner 
            and Janne Heikkila and Tony Tung},
    year={2021},
    eprint={2112.13889},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">                
                    <br>
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
