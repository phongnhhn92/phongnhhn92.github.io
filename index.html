<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Phong Nguyen</title>
  
  <meta name="author" content="Phong Nguyen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="#">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Phong Nguyen</name>
              </p>
              <p style="text-align:justify"> I am a PhD student at Center For Machine Vision and Signal Analysis (<a href="https://www.oulu.fi/cmvs/">CMVS</a>), University of Oulu, Finland, where I am co-advised by 
                <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Prof. Janne Heikkilä</a> and <a href="https://esa.rahtu.fi/">Prof. Esa Rahtu.</a>                                
              </p>
              <p style="text-align:justify">
                I have a MS in Electronics and Electrical Engineering (Autonomous AI Drone) at Dongguk University, South Korea where I was a research assistant for <a href="http://dm.dgu.edu/index.html">Prof.Kang Ryoung Park</a>. I have a BS in Mechanical Engineering from <a href="https://en.hust.edu.vn/">HUST</a>, Vietnam.
              </p>
              <p style="text-align:justify">
                From May to November of 2021, I joined the <a href="https://about.facebook.com/realitylabs/">Reality Labs Research, Sausalito</a> where I was a research intern for <a href="https://nsarafianos.github.io/">Nikolaos Sarafianos</a>, <a href="https://christophlassner.de/">Christoph Lassner</a> and <a href="https://sites.google.com/site/tony2ng/">Tony Tung</a>. 
              I am also very lucky to have a 2022 summer internship at <a href="https://nv-tlabs.github.io/">NVIDIA Toronto AI Lab</a> and work with  <a href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a> and <a href="https://www.samehkhamis.com/">Sameh Khamis</a>.         </p>
              <p style="text-align:center">
                <a href="mailto:phong.nguyen@oulu.fi">Email</a> &nbsp/&nbsp
                <a href="data/myCV.pdf">CV</a> &nbsp/&nbsp          
                <a href="https://scholar.google.com/citations?user=030WAOoAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/PhongStormVN">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/phongnhhn92">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:20%;max-width:20%">
              <a href="images/profile.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading ><strong style="font-size:20px">Research</strong></heading>
            <p>
              I'm interested in the topic of 3D reconstruction, novel view synthesis and neural rendering. My research combines 3D computer vision and deep learning.
            </p>            
          </td>
        </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>          
          <tr onmouseout="hvsnet_stop()" onmouseover="hvsnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hvsnet_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/6_HVSNet/out.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/6_HVSNet/static.png' width="160">
              </div>
              <script type="text/javascript">
                function hvsnet_start() {
                  document.getElementById('hvsnet_image').style.opacity = "1";
                }

                function hvsnet_stop() {
                  document.getElementById('hvsnet_image').style.opacity = "0";
                }
                hvsnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="HVS_Net/index.html">
                <papertitle>Free-Viewpoint RGB-D Human Performance Capture and Rendering</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>,
              <a href="https://nsarafianos.github.io/">Nikolaos Sarafianos</a>,
              <a href="https://christophlassner.de/">Christoph Lassner</a>,
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkilä</a>,        
              <a href="https://sites.google.com/site/tony2ng/">Tony Tung</a>
              <br>
							<em>ECCV</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2112.13889">arxiv</a> /
              <a href="data/bib/hvs_net.bib">bibtex</a> /
              <a href="HVS_Net/index.html">project page</a>
              <p></p>
              <p style="text-align:justify">
                We propose an architecture to learn dense features in novel views obtained by sphere-based neural rendering, and create complete renders using a global context inpainting model.
Additionally, an enhancer network leverages the overall fidelity, even in occluded areas from the original view, producing crisp renders with fine details. Our method produces high quality novel images and generalizes on unseen human actors during inferences.
							</p>
            </td>
          </tr>


          <tr onmouseout="rgbd_stop()" onmouseover="rgbd_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='rgbd_image'><video  width=100% height=75% muted autoplay loop>
                <source src="images/5_RGBDNet/room.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/5_RGBDNet/00075.jpg' width="160">
              </div>
              <script type="text/javascript">
                function rgbd_start() {
                  document.getElementById('rgbd_image').style.opacity = "1";
                }

                function rgbd_stop() {
                  document.getElementById('rgbd_image').style.opacity = "0";
                }
                rgbd_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2011.14398">
                <papertitle>RGBD-Net: Predicting Color and Depth images for Novel Views Synthesis</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>,
              <a href="https://akanimax.github.io/">Animesh Karnewar</a>,
              <a href="https://lamhuynh.info/">Lam Huynh</a>,
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,        
              <a href="https://cmp.felk.cvut.cz/~matas/">Jiri Matas</a>,    
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkila</a> 
              <br>
							<em>3DV</em>, 2021
              <br>
              <a href="https://github.com/phongnhhn92/RGBDNet">code</a> /
              <a href="data/bib/3dv.bib">bibtex</a> /
              <a href="https://recorder-v3.slideslive.com/?share=56374&s=66606cbd-dc42-4e04-9114-a095a76efbd5">video</a> /
              <a href="https://www.dropbox.com/scl/fi/50fnwjmoy8w4x2ljnbdgr/3DV_10min_slide.pptx?dl=0&rlkey=20jbywhk5gv5r7fr4b7sxazmm">slides</a>
              <p></p>
              <p style="text-align:justify">
                We propose a new cascaded architecture for novel view synthesis, called <strong>RGBD-Net</strong>, which consists of two core components: a hierarchical depth regression network and a depth-aware generator network. The former one predicts depth maps of the target views by using adaptive depth scaling, while the latter one leverages the predicted depths and renders spatially and temporally consistent target images.
							</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='PF_image'>
                  <img src='images/lidnas/LiDNAS.gif' width="160" height = "160"></div>                
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2108.11105.pdf">
                <papertitle>Lightweight Monocular Depth with a Novel Neural Architecture Search Method </papertitle>
              </a>
              <br>
              <a href="https://lamhuynh.info/">Lam Huynh</a>,              
              <strong>Phong Nguyen</strong>,              
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,     
              <a href="https://cmp.felk.cvut.cz/~matas/">Jiri Matas</a>,         
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkila</a>              
              <br>
							<em>WACV</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2108.11105.pdf">arxiv</a> /
              <a href="data/bib/lidnas.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                This paper presents a novel neural architecture search method, called LiDNAS, for generating lightweight monocular depth estimation models. Unlike previous neural architecture search (NAS) approaches, where finding optimized networks are computationally highly demanding, the introduced novel Assisted Tabu Search leads to efficient architecture exploration.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='PF_image'>
                  <img src='images/HessianLoss/FuSaNet.gif' width="160" height = "160"></div>                
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2108.11098.pdf">
                <papertitle>Monocular Depth Estimation Primed by Salient Point Detection and Hessian Loss</papertitle>
              </a>
              <br>
              <a href="https://lamhuynh.info/">Lam Huynh</a>,
              <a href="https://scholar.google.fi/citations?user=4b03v-IAAAAJ&hl=en">Matteo Pedone</a>,
              <strong>Phong Nguyen</strong>,              
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,     
              <a href="https://cmp.felk.cvut.cz/~matas/">Jiri Matas</a>,         
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkila</a>              
              <br>
							<em>3DV</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2108.11098.pdf">arxiv</a> /
              <a href="data/bib/HessianLoss.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                This work proposes an accurate and lightweight framework for monocular depth estimation based on a self-attention mechanism stemming from salient point detection. Specifically, we utilize a sparse set of keypoints to train a FuSaNet model that consists of two major components: Fusion-Net and Saliency-Net.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='PF_image'>
                  <img src='images/Point_fusion/point_fusion.gif' width="160" height = "160"></div>                
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2012.10296.pdf">
                <papertitle>Boosting Monocular Depth Estimation with Lightweight 3D Point Fusion</papertitle>
              </a>
              <br>
              <a href="https://lamhuynh.info/">Lam Huynh</a>,
              <strong>Phong Nguyen</strong>,              
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,     
              <a href="https://cmp.felk.cvut.cz/~matas/">Jiri Matas</a>,         
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkila</a>              
              <br>
							<em>ICCV</em>, 2021
              <br>
              <a href="https://huynhlam.github.io/3d-point-fusion/">project page</a> /
              <a href="https://arxiv.org/pdf/2012.10296.pdf">arxiv</a> /
              <a href="data/bib/point_fusion.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                In this paper, we propose enhancing monocular depth estimation by adding 3D points as depth guidance. Unlike existing depth completion methods, our approach performs well on extremely sparse and unevenly distributed point clouds, which makes it agnostic to the source of the 3D points.
              </p>
            </td>
          </tr>

          <tr onmouseout="accv_stop()" onmouseover="accv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='accv_image'>
                  <img src='images/4_TGQN/1.png' width="160"></div>
                <img src='images/4_TGQN/2.png' width="160">
              </div>
              <script type="text/javascript">
                function accv_start() {
                  document.getElementById('accv_image').style.opacity = "1";
                }

                function accv_stop() {
                  document.getElementById('accv_image').style.opacity = "0";
                }
                accv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Nguyen-Ha_Sequential_View_Synthesis_with_Transformer_ACCV_2020_paper.pdf">
                <papertitle>Sequential View Synthesis with Transformer</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>,
              <a href="https://lamhuynh.info/">Lam Huynh</a>,
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,              
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkila</a>              
              <br>
							<em>ACCV</em>, 2020
              <br>
              <a href="data/bib/accv.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                We introduces Transformer-based Generative Query Network (T-GQN) which uses multi-view attention learning between context images to obtain multiple implicit scene representations. A sequential rendering decoder is presented to predict multiple target images, based on the learned representations. T-GQN not only gives consistent predictions
                but also doesn’t require any retraining for finetuning.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dav_image'>
                  <img src='images/DAV/dav.gif' width="160" height = "160"></div>                
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-58574-7_35">
                <papertitle>Guiding Monocular Depth Estimation Using Depth-Attention Volume</papertitle>
              </a>
              <br>
              <a href="https://lamhuynh.info/">Lam Huynh</a>,
              <strong>Phong Nguyen</strong>,              
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,     
              <a href="https://cmp.felk.cvut.cz/~matas/">Jiri Matas</a>,         
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkila</a>              
              <br>
							<em>ECCV</em>, 2020
              <br>
              <a href="https://huynhlam.github.io/mono-depth-DAV/">project page</a> /
              <a href="https://arxiv.org/abs/2004.02760">arxiv</a> /
              <a href="data/bib/dav.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                In this paper, we propose guiding depth estimation to favor planar structures that are ubiquitous especially in indoor environments. This is achieved by incorporating a non-local coplanarity constraint to the network with a novel attention mechanism called depth-attention volume (DAV).
              </p>
            </td>
          </tr>
          

          <tr onmouseout="gaqn_stop()" onmouseover="gaqn_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='gaqn_image'>
                  <img src='images/3_GAQN/1.png' width="160"></div>
                <img src='images/3_GAQN/2.png' width="160">
              </div>
              <script type="text/javascript">
                function gaqn_start() {
                  document.getElementById('gaqn_image').style.opacity = "1";
                }

                function gaqn_stop() {
                  document.getElementById('gaqn_image').style.opacity = "0";
                }
                gaqn_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1904.05124">
                <papertitle>Predicting Novel Views Using Generative Adversarial Query Network</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>,
              <a href="https://lamhuynh.info/">Lam Huynh</a>,
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,              
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkila</a>              
              <br>
							<em>SCIA</em>, 2019 (<u>Best Paper Award</u>)
              <br>
              <a href="data/bib/scia.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                We introduces the Generative Adversarial Query Network (GAQN), a general learning framework for novel view synthesis that combines Generative Query Network (GQN) and Generative Adversarial Networks (GANs).
              </p>
            </td>
          </tr>

          <tr onmouseout="npil1_stop()" onmouseover="npil1_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil1_image'>
                  <img src='images/2_lightDenseYOLo/1.png' width="160"></div>
                <img src='images/2_lightDenseYOLo/2.png' width="110">
              </div>
              <script type="text/javascript">
                function npil1_start() {
                  document.getElementById('npil1_image').style.opacity = "1";
                }

                function npil1_stop() {
                  document.getElementById('npil1_image').style.opacity = "0";
                }
                npil1_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.mdpi.com/1424-8220/18/6/1703">
                <papertitle>LightDenseYOLO: A Fast and Accurate Marker Tracker for Autonomous UAV Landing by Visible Light Camera Sensor on Drone</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>,
              Muhammad Arsalan,Ja Hyung Koo, Rizwan Ali Naqvi, Noi Quang Truong
              <a href="http://dm.dgu.edu/index.html">Kang Ryoung Park</a>              
              <br>
							<em>Sensors</em>, 2018
              <br>
              <a href="data/bib/sensors-v18-i06_20211201.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                We proposed lightDenseYOLO, a novel marker detector for autonomous drone landing using deep neural networks.
              </p>
            </td>
          </tr>

          <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
                  <img src='images/1_markerPaper/sensors-17-01987-g004-550.png' width="160"></div>
                <img src='images/1_markerPaper/2021-12-01_16-19.png' width="160">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.mdpi.com/1424-8220/17/9/1987">
                <papertitle>Remote Marker-Based Tracking for UAV Landing Using Visible-Light Camera Sensor</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>
              Ki Wan Kim, 
              Young Won Lee,
              <a href="http://dm.dgu.edu/index.html">Kang Ryoung Park</a>              
              <br>
							<em>Sensors</em>, 2017
              <br>
              <a href="data/bib/sensors-v17-i09_20211201.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                In this research, we determined how to safely land a drone in the absence of GPS signals using our remote maker-based tracking algorithm based on the visible light camera sensor.
              </p>
            </td>
          </tr>          
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading><strong style="font-size:20px">Reading Group for Vietnamese</strong> </heading>
            <p>
              During my free time, I made explaining videos for exciting computer vision papers at the <a href="https://www.youtube.com/channel/UCiP4GqgAKpspuDEkYVYgbfQ">Cracking Papers 4 VN</a> Youtube channel. Here are some examples:               
            </p>
            <p style="text-align:justify">
              <iframe style="text-align:justify" width="560" height="315" src="https://www.youtube.com/embed/UN5yXZY2cS0" title="Perceiver and Perceiver IO" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> 
            </p>
            <p style="text-align:justify">
              <iframe style="text-align:justify" width="560" height="315" src="https://www.youtube.com/embed/eNDKK_AWKSk" title="Review: Heavy Rain Image Restoration" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </p>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                The credit of this website template goes to <a href="https://jonbarron.info/">Jon Barron</a>. Thank you!
                <!-- Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page. -->
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
