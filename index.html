<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Phong Nguyen</title>
  
  <meta name="author" content="Phong Nguyen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="#">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Phong Nguyen</name>
              </p>
              <p style="text-align:justify">I am a PhD in Computer Science graduated from University of Oulu, Finland. I am currently a Research Scientist at <a href="https://www.vinai.io/">VinAI</a> in Ha Noi, Viet Nam. Before, I was a Postdoc Researcher at Center For Machine Vision and Signal Analysis (<a href="https://www.oulu.fi/cmvs/">CMVS</a>), University of Oulu, Finland, where I am co-advised by 
                <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Prof. Janne Heikkilä</a> and <a href="https://esa.rahtu.fi/">Prof. Esa Rahtu.</a>  I also worked remotely as a Research Scientist at <a href="https://spreeai.com/">SpreeAI</a> - a US startup company on virtual try-on with <a href="https://minhpvo.github.io/">Dr. Minh Vo</a>.
              </p>
              <p style="text-align:justify">
                I have a MS in Electronics and Electrical Engineering (Autonomous AI Drone) at Dongguk University, South Korea where I was a research assistant for <a href="http://dm.dgu.edu/index.html">Prof.Kang Ryoung Park</a>. I have a BS in Mechanical Engineering from <a href="https://en.hust.edu.vn/">HUST</a>, Vietnam.
              </p>
              <p style="text-align:justify">
                From May to November of 2021, I joined the <a href="https://about.facebook.com/realitylabs/">Reality Labs Research, Sausalito</a> where I was a research intern for <a href="https://nsarafianos.github.io/">Nikolaos Sarafianos</a>, <a href="https://christophlassner.de/">Christoph Lassner</a> and <a href="https://sites.google.com/site/tony2ng/">Tony Tung</a>. 
              I am also very lucky to have a 2022 summer internship at <a href="https://nv-tlabs.github.io/">NVIDIA Toronto AI Lab</a> and work with  <a href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a> and <a href="https://www.samehkhamis.com/">Sameh Khamis</a>.         </p>

              <p style="text-align:center">
                <a href="mailto:phongnhhn92@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/MyCV.pdf">CV</a> &nbsp/&nbsp          
                <a href="https://scholar.google.com/citations?user=030WAOoAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/PhongStormVN">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/phongnhhn92">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:20%;max-width:20%">
              <a href="images/profile.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading ><strong style="font-size:20px">Research</strong></heading>
            <p>
              I'm interested in the topic of 3D reconstruction, novel view synthesis and 2D/3D neural rendering. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance field
            </p>            
          </td>
        </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>          
          <tr onmouseout="dream_stop()" onmouseover="dream_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dream'>
                  <img src='images/8_dream/dream.gif' width="160"></div>  
                <img src='images/8_dream/dream.png' width="160">
              </div>
              <script type="text/javascript">
                function dream_start() {
                  document.getElementById('dream').style.opacity = "1";
                }

                function dream_stop() {
                  document.getElementById('dream').style.opacity = "0";
                }
                dream_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.02192">
                <papertitle>DiverseDream: Diverse Text-to-3D Synthesis with Augmented Text Embedding</papertitle>
              </a>
              <br>
              <a>Uy Dieu Tran</a>,
              <a>Minh Luu</a>,        
              <strong>Phong Nguyen</strong>,
              <a href="https://www.khoinguyen.org/">Khoi Nguyen</a>,
              <a href="https://sonhua.github.io/">Binh-Son Hua</a>
              <br>
							<em>ECCV</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2312.02192v2">arxiv</a> /
              <a href="https://github.com/VinAIResearch/DiverseDream">code</a> /
              <a href="https://diversedream.github.io/">project page</a> /
              <a href="data/bib/diverse_dream.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                An intriguing but underexplored problem with existing text-to-3D methods is that 3D models obtained from the sampling-by-optimization procedure tend to have mode collapses, and hence poor diversity in their results. In this paper, we provide an analysis and identify potential causes of such a limited diversity, and then devise a new method that considers the joint generation of different 3D models from the same text prompt, where we propose to use augmented text prompts via textual inversion of reference images to diversify the joint generation.
							</p>
            </td>
          </tr>
          
          
          <tr onmouseout="cas_tpami_stop()" onmouseover="cas_tpami_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cas_tpami'>
                  <img src='images/7_cas_tpami/cas_tpami.gif' width="160"></div>  
                <img src='images/7_cas_tpami/cas_tpami.png' width="160">
              </div>
              <script type="text/javascript">
                function cas_tpami_start() {
                  document.getElementById('cas_tpami').style.opacity = "1";
                }

                function cas_tpami_stop() {
                  document.getElementById('cas_tpami').style.opacity = "0";
                }
                cas_tpami_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.computer.org/csdl/journal/tp/5555/01/10328666/1SkORui8VIk">
                <papertitle>Cascaded and Generalizable Neural Radiance Fields for Fast View Synthesis</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>,
              <a href="https://lamhuynh.info/">Lam Huynh</a>,
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,        
              <a href="https://cmp.felk.cvut.cz/~matas/">Jiri Matas</a>,
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkilä</a>      
              <br>
							<em>TPAMI</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2208.04717">arxiv</a> /
              <a href="data/bib/cas_tpami.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                We present CG-NeRF, a cascade and generalizable neural radiance fields method for view synthesis. Our approach addresses the problems of fast and generalizing view synthesis by proposing two novel modules: a coarse radiance fields predictor and a convolutional-based neural renderer. This architecture infers consistent scene geometry based on the implicit neural fields and renders new views efficiently using a single GPU.
                Moreover, our method can leverage a denser set of reference images of a single scene to produce accurate novel views without relying on additional explicit representations and still maintains the high-speed rendering of the pre-trained model.

							</p>
            </td>
          </tr>

          <tr onmouseout="thesis_stop()" onmouseover="thesis_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='thesis'>
                  <img src='images/10_thesis/1.png' width="160"></div>
                <img src='images/10_thesis/2.png' width="160">
              </div>
              <script type="text/javascript">
                function thesis_start() {
                  document.getElementById('thesis').style.opacity = "1";
                }

                function thesis_stop() {
                  document.getElementById('thesis').style.opacity = "0";
                }
                thesis_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.computer.org/csdl/journal/tp/5555/01/10328666/1SkORui8VIk">
                <papertitle>Neural scene representations for learning-based view Synthesis</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>    
              <br>
							<em>PhD Thesis</em>, 2023
              <br>
              <a href="https://oulurepo.oulu.fi/handle/10024/46361"">link</a> 
              <p></p>
              <p style="text-align:justify">
                This thesis introduces learning-based novel view Synthesis approaches using different neural scene representations. Traditional representations, such as voxels or point clouds, are often computationally expensive and challenging to work with. Neural scene representations, on the other hand, can be more compact and efficient, allowing faster processing and better performance. Additionally, neural scene representations can be learned end-to-end from data, enabling them to be adapted to specific tasks and domains.
							</p>
            </td>
          </tr>
          
          
          <tr onmouseout="hvsnet_stop()" onmouseover="hvsnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hvsnet_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/6_HVSNet/out.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/6_HVSNet/static.png' width="160">
              </div>
              <script type="text/javascript">
                function hvsnet_start() {
                  document.getElementById('hvsnet_image').style.opacity = "1";
                }

                function hvsnet_stop() {
                  document.getElementById('hvsnet_image').style.opacity = "0";
                }
                hvsnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="HVS_Net/index.html">
                <papertitle>Free-Viewpoint RGB-D Human Performance Capture and Rendering</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>,
              <a href="https://nsarafianos.github.io/">Nikolaos Sarafianos</a>,
              <a href="https://christophlassner.de/">Christoph Lassner</a>,
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkilä</a>,        
              <a href="https://sites.google.com/site/tony2ng/">Tony Tung</a>
              <br>
							<em>ECCV</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2112.13889">arxiv</a> /
              <a href="data/bib/hvs_net.bib">bibtex</a> /
              <a href="HVS_Net/index.html">project page</a> /
              <a href="HVS_Net/img/poster_eccv2022.pdf">poster</a> /
              <a href="Hhttps://youtu.be/49pevNjgrLc">video</a>
              <p></p>
              <p style="text-align:justify">
                We propose an architecture to learn dense features in novel views obtained by sphere-based neural rendering, and create complete renders using a global context inpainting model.
Additionally, an enhancer network leverages the overall fidelity, even in occluded areas from the original view, producing crisp renders with fine details. Our method produces high quality novel images and generalizes on unseen human actors during inferences.
							</p>
            </td>
          </tr>


          <tr onmouseout="rgbd_stop()" onmouseover="rgbd_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='rgbd_image'><video  width=100% height=75% muted autoplay loop>
                <source src="images/5_RGBDNet/room.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/5_RGBDNet/00075.jpg' width="160">
              </div>
              <script type="text/javascript">
                function rgbd_start() {
                  document.getElementById('rgbd_image').style.opacity = "1";
                }

                function rgbd_stop() {
                  document.getElementById('rgbd_image').style.opacity = "0";
                }
                rgbd_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2011.14398">
                <papertitle>RGBD-Net: Predicting Color and Depth images for Novel Views Synthesis</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>,
              <a href="https://akanimax.github.io/">Animesh Karnewar</a>,
              <a href="https://lamhuynh.info/">Lam Huynh</a>,
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,        
              <a href="https://cmp.felk.cvut.cz/~matas/">Jiri Matas</a>,    
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkila</a> 
              <br>
							<em>3DV</em>, 2021
              <br>
              <a href="https://github.com/phongnhhn92/RGBDNet">code</a> /
              <a href="data/bib/3dv.bib">bibtex</a> /
              <a href="https://recorder-v3.slideslive.com/?share=56374&s=66606cbd-dc42-4e04-9114-a095a76efbd5">video</a> /
              <!-- <a href="https://www.dropbox.com/scl/fi/50fnwjmoy8w4x2ljnbdgr/3DV_10min_slide.pptx?dl=0&rlkey=20jbywhk5gv5r">slides</a> -->
              <p></p>
              <p style="text-align:justify">
                We propose a new cascaded architecture for novel view synthesis, called <strong>RGBD-Net</strong>, which consists of two core components: a hierarchical depth regression network and a depth-aware generator network. The former one predicts depth maps of the target views by using adaptive depth scaling, while the latter one leverages the predicted depths and renders spatially and temporally consistent target images.
							</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='PF_image'>
                  <img src='images/lidnas/LiDNAS.gif' width="160" height = "160"></div>                
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2108.11105.pdf">
                <papertitle>Lightweight Monocular Depth with a Novel Neural Architecture Search Method </papertitle>
              </a>
              <br>
              <a href="https://lamhuynh.info/">Lam Huynh</a>,              
              <strong>Phong Nguyen</strong>,              
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,     
              <a href="https://cmp.felk.cvut.cz/~matas/">Jiri Matas</a>,         
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkila</a>              
              <br>
							<em>WACV</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2108.11105.pdf">arxiv</a> /
              <a href="data/bib/lidnas.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                This paper presents a novel neural architecture search method, called LiDNAS, for generating lightweight monocular depth estimation models. Unlike previous neural architecture search (NAS) approaches, where finding optimized networks are computationally highly demanding, the introduced novel Assisted Tabu Search leads to efficient architecture exploration.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='PF_image'>
                  <img src='images/HessianLoss/FuSaNet.gif' width="160" height = "160"></div>                
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2108.11098.pdf">
                <papertitle>Monocular Depth Estimation Primed by Salient Point Detection and Hessian Loss</papertitle>
              </a>
              <br>
              <a href="https://lamhuynh.info/">Lam Huynh</a>,
              <a href="https://scholar.google.fi/citations?user=4b03v-IAAAAJ&hl=en">Matteo Pedone</a>,
              <strong>Phong Nguyen</strong>,              
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,     
              <a href="https://cmp.felk.cvut.cz/~matas/">Jiri Matas</a>,         
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkila</a>              
              <br>
							<em>3DV</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2108.11098.pdf">arxiv</a> /
              <a href="data/bib/HessianLoss.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                This work proposes an accurate and lightweight framework for monocular depth estimation based on a self-attention mechanism stemming from salient point detection. Specifically, we utilize a sparse set of keypoints to train a FuSaNet model that consists of two major components: Fusion-Net and Saliency-Net.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='PF_image'>
                  <img src='images/Point_fusion/point_fusion.gif' width="160" height = "160"></div>                
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2012.10296.pdf">
                <papertitle>Boosting Monocular Depth Estimation with Lightweight 3D Point Fusion</papertitle>
              </a>
              <br>
              <a href="https://lamhuynh.info/">Lam Huynh</a>,
              <strong>Phong Nguyen</strong>,              
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,     
              <a href="https://cmp.felk.cvut.cz/~matas/">Jiri Matas</a>,         
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkila</a>              
              <br>
							<em>ICCV</em>, 2021
              <br>
              <a href="https://huynhlam.github.io/3d-point-fusion/">project page</a> /
              <a href="https://arxiv.org/pdf/2012.10296.pdf">arxiv</a> /
              <a href="data/bib/point_fusion.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                In this paper, we propose enhancing monocular depth estimation by adding 3D points as depth guidance. Unlike existing depth completion methods, our approach performs well on extremely sparse and unevenly distributed point clouds, which makes it agnostic to the source of the 3D points.
              </p>
            </td>
          </tr>

          <tr onmouseout="accv_stop()" onmouseover="accv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='accv_image'>
                  <img src='images/4_TGQN/1.png' width="160"></div>
                <img src='images/4_TGQN/2.png' width="160">
              </div>
              <script type="text/javascript">
                function accv_start() {
                  document.getElementById('accv_image').style.opacity = "1";
                }

                function accv_stop() {
                  document.getElementById('accv_image').style.opacity = "0";
                }
                accv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Nguyen-Ha_Sequential_View_Synthesis_with_Transformer_ACCV_2020_paper.pdf">
                <papertitle>Sequential View Synthesis with Transformer</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>,
              <a href="https://lamhuynh.info/">Lam Huynh</a>,
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,              
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkila</a>              
              <br>
							<em>ACCV</em>, 2020
              <br>
              <a href="data/bib/accv.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                We introduces Transformer-based Generative Query Network (T-GQN) which uses multi-view attention learning between context images to obtain multiple implicit scene representations. A sequential rendering decoder is presented to predict multiple target images, based on the learned representations. T-GQN not only gives consistent predictions
                but also doesn’t require any retraining for finetuning.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dav_image'>
                  <img src='images/DAV/dav.gif' width="160" height = "160"></div>                
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-58574-7_35">
                <papertitle>Guiding Monocular Depth Estimation Using Depth-Attention Volume</papertitle>
              </a>
              <br>
              <a href="https://lamhuynh.info/">Lam Huynh</a>,
              <strong>Phong Nguyen</strong>,              
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,     
              <a href="https://cmp.felk.cvut.cz/~matas/">Jiri Matas</a>,         
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkila</a>              
              <br>
							<em>ECCV</em>, 2020
              <br>
              <a href="https://huynhlam.github.io/mono-depth-DAV/">project page</a> /
              <a href="https://arxiv.org/abs/2004.02760">arxiv</a> /
              <a href="data/bib/dav.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                In this paper, we propose guiding depth estimation to favor planar structures that are ubiquitous especially in indoor environments. This is achieved by incorporating a non-local coplanarity constraint to the network with a novel attention mechanism called depth-attention volume (DAV).
              </p>
            </td>
          </tr>
          

          <tr onmouseout="gaqn_stop()" onmouseover="gaqn_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='gaqn_image'>
                  <img src='images/3_GAQN/1.png' width="160"></div>
                <img src='images/3_GAQN/2.png' width="160">
              </div>
              <script type="text/javascript">
                function gaqn_start() {
                  document.getElementById('gaqn_image').style.opacity = "1";
                }

                function gaqn_stop() {
                  document.getElementById('gaqn_image').style.opacity = "0";
                }
                gaqn_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1904.05124">
                <papertitle>Predicting Novel Views Using Generative Adversarial Query Network</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>,
              <a href="https://lamhuynh.info/">Lam Huynh</a>,
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,              
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkila</a>              
              <br>
							<em>SCIA</em>, 2019 (<u>Best Paper Award</u>)
              <br>
              <a href="data/bib/scia.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                We introduces the Generative Adversarial Query Network (GAQN), a general learning framework for novel view synthesis that combines Generative Query Network (GQN) and Generative Adversarial Networks (GANs).
              </p>
            </td>
          </tr>

          <tr onmouseout="npil1_stop()" onmouseover="npil1_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil1_image'>
                  <img src='images/2_lightDenseYOLo/1.png' width="160"></div>
                <img src='images/2_lightDenseYOLo/2.png' width="110">
              </div>
              <script type="text/javascript">
                function npil1_start() {
                  document.getElementById('npil1_image').style.opacity = "1";
                }

                function npil1_stop() {
                  document.getElementById('npil1_image').style.opacity = "0";
                }
                npil1_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.mdpi.com/1424-8220/18/6/1703">
                <papertitle>LightDenseYOLO: A Fast and Accurate Marker Tracker for Autonomous UAV Landing by Visible Light Camera Sensor on Drone</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>,
              Muhammad Arsalan,Ja Hyung Koo, Rizwan Ali Naqvi, Noi Quang Truong
              <a href="http://dm.dgu.edu/index.html">Kang Ryoung Park</a>              
              <br>
							<em>Sensors</em>, 2018
              <br>
              <a href="data/bib/sensors-v18-i06_20211201.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                We proposed lightDenseYOLO, a novel marker detector for autonomous drone landing using deep neural networks.
              </p>
            </td>
          </tr>

          <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
                  <img src='images/1_markerPaper/sensors-17-01987-g004-550.png' width="160"></div>
                <img src='images/1_markerPaper/2021-12-01_16-19.png' width="160">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.mdpi.com/1424-8220/17/9/1987">
                <papertitle>Remote Marker-Based Tracking for UAV Landing Using Visible-Light Camera Sensor</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>
              Ki Wan Kim, 
              Young Won Lee,
              <a href="http://dm.dgu.edu/index.html">Kang Ryoung Park</a>              
              <br>
							<em>Sensors</em>, 2017
              <br>
              <a href="data/bib/sensors-v17-i09_20211201.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                In this research, we determined how to safely land a drone in the absence of GPS signals using our remote maker-based tracking algorithm based on the visible light camera sensor.
              </p>
            </td>
          </tr>          
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading><strong style="font-size:20px">Reading Group for Vietnamese</strong> </heading>
            <p>
              During my free time, I made explaining videos for exciting computer vision papers at the <a href="https://www.youtube.com/channel/UCiP4GqgAKpspuDEkYVYgbfQ">Cracking Papers 4 VN</a> Youtube channel. Here are some examples:               
            </p>
            <p style="text-align:justify">
              <iframe style="text-align:justify" width="560" height="315" src="https://www.youtube.com/embed/UN5yXZY2cS0" title="Perceiver and Perceiver IO" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> 
            </p>
            <p style="text-align:justify">
              <iframe style="text-align:justify" width="560" height="315" src="https://www.youtube.com/embed/eNDKK_AWKSk" title="Review: Heavy Rain Image Restoration" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </p>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                The credit of this website template goes to <a href="https://jonbarron.info/">Jon Barron</a>. Thank you!
                <!-- Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page. -->
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
