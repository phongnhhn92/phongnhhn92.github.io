<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Phong Nguyen</title>
  
  <meta name="author" content="Phong Nguyen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Phong Nguyen</name>
              </p>
              <p style="text-align:justify"> I am a PhD student at Center For Machine Vision and Signal Analysis (<a href="https://www.oulu.fi/cmvs/">CMVS</a>), University of Oulu, Finland, where I am co-advised by 
                <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Prof. Janne Heikkilä</a> and <a href="https://esa.rahtu.fi/">Prof. Esa Rahtu.</a>                                
              </p>
              <p style="text-align:justify">
                I have a MS in Electronics and Electrical Engineering (Autonomous AI Drone) at Dongguk University, South Korea where I was a research assistant for <a href="http://dm.dgu.edu/index.html">Prof.Kang Ryoung Park</a>. I have a BS in Mechanical Engineering from <a href="https://en.hust.edu.vn/">HUST</a>, Vietnam.
              </p>
              <p style="text-align:justify">
                From May to November of 2021, I joined the <a href="https://about.facebook.com/realitylabs/">Reality Labs Research, Sausalito</a> where I was a research intern for <a href="https://nsarafianos.github.io/">Nikolaos Sarafianos</a>,<a href="https://christophlassner.de/">Christoph Lassner</a> and <a href="https://sites.google.com/site/tony2ng/">Tony Tung</a>.              </p>
              <p style="text-align:center">
                <a href="mailto:phong.nguyen@oulu.fi">Email</a> &nbsp/&nbsp
                <a href="data/PhongNguyen_CV.pdf">CV</a> &nbsp/&nbsp          
                <a href="https://scholar.google.com/citations?user=030WAOoAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/PhongStormVN">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/phongnhhn92">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in the topic of 3D reconstruction, novel view synthesis and neural rendering. My research combines 3D computer vision and deep learning.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr onmouseout="rgbd_stop()" onmouseover="rgbd_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='rgbd_image'><video  width=100% height=75% muted autoplay loop>
                <source src="images/5_RGBDNet/room.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/5_RGBDNet/00075.jpg' width="160">
              </div>
              <script type="text/javascript">
                function rgbd_start() {
                  document.getElementById('rgbd_image').style.opacity = "1";
                }

                function rgbd_stop() {
                  document.getElementById('rgbd_image').style.opacity = "0";
                }
                rgbd_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2011.14398">
                <papertitle>RGBD-Net: Predicting Color and Depth images for Novel Views Synthesis</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>
              <a href="https://akanimax.github.io/">Animesh Karnewar</a>,
              <a href="https://lamhuynh.info/">Lam Huynh</a>,
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,        
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkila</a> 
              <br>
							<em>3DV</em>, 2021
              <br>
              <a href="data/bib/3dv.bib">bibtex</a> /
              <a href="https://recorder-v3.slideslive.com/?share=56374&s=66606cbd-dc42-4e04-9114-a095a76efbd5">video</a>
              <p></p>
              <p style="text-align:justify">
                We propose a new cascaded architecture for novel view synthesis, called <strong>RGBD-Net</strong>, which consists of two core components: a hierarchical depth regression network and a depth-aware generator network. The former one predicts depth maps of the target views by using adaptive depth scaling, while the latter one leverages the predicted depths and renders spatially and temporally consistent target images.
							</p>
            </td>
          </tr>

          <tr onmouseout="accv_stop()" onmouseover="accv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='accv_image'>
                  <img src='images/4_TGQN/1.png' width="160"></div>
                <img src='images/4_TGQN/2.png' width="160">
              </div>
              <script type="text/javascript">
                function accv_start() {
                  document.getElementById('accv_image').style.opacity = "1";
                }

                function accv_stop() {
                  document.getElementById('accv_image').style.opacity = "0";
                }
                accv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Nguyen-Ha_Sequential_View_Synthesis_with_Transformer_ACCV_2020_paper.pdf">
                <papertitle>Sequential View Synthesis with Transformer</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>
              <a href="https://lamhuynh.info/">Lam Huynh</a>,
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,              
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkila</a>              
              <br>
							<em>ACCV</em>, 2020
              <br>
              <a href="data/bib/accv.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                We introduces Transformer-based Generative Query Network (T-GQN) which uses multi-view attention learning between context images to obtain multiple implicit scene representations. A sequential rendering decoder is presented to predict multiple target images, based on the learned representations. T-GQN not only gives consistent predictions
                but also doesn’t require any retraining for finetuning.
              </p>
            </td>
          </tr>

          <tr onmouseout="gaqn_stop()" onmouseover="gaqn_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='gaqn_image'>
                  <img src='images/3_GAQN/1.png' width="160"></div>
                <img src='images/3_GAQN/2.png' width="160">
              </div>
              <script type="text/javascript">
                function gaqn_start() {
                  document.getElementById('gaqn_image').style.opacity = "1";
                }

                function gaqn_stop() {
                  document.getElementById('gaqn_image').style.opacity = "0";
                }
                gaqn_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1904.05124">
                <papertitle>Predicting Novel Views Using Generative Adversarial Query Network</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>
              <a href="https://lamhuynh.info/">Lam Huynh</a>,
              <a href="https://esa.rahtu.fi/">Esa Rahtu</a>,              
              <a href="https://www.oulu.fi/university/researcher/janne-heikkila">Janne Heikkila</a>              
              <br>
							<em>SCIA</em>, 2019 (<u>Best Paper Award</u>)
              <br>
              <a href="data/bib/scia.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                We introduces the Generative Adversarial Query Network (GAQN), a general learning framework for novel view synthesis that combines Generative Query Network (GQN) and Generative Adversarial Networks (GANs)
              </p>
            </td>
          </tr>

          <tr onmouseout="npil1_stop()" onmouseover="npil1_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil1_image'>
                  <img src='images/2_lightDenseYOLo/1.png' width="160"></div>
                <img src='images/2_lightDenseYOLo/2.png' width="110">
              </div>
              <script type="text/javascript">
                function npil1_start() {
                  document.getElementById('npil1_image').style.opacity = "1";
                }

                function npil1_stop() {
                  document.getElementById('npil1_image').style.opacity = "0";
                }
                npil1_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.mdpi.com/1424-8220/18/6/1703">
                <papertitle>LightDenseYOLO: A Fast and Accurate Marker Tracker for Autonomous UAV Landing by Visible Light Camera Sensor on Drone</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>,
              Muhammad Arsalan,Ja Hyung Koo, Rizwan Ali Naqvi, Noi Quang Truong
              <a href="http://dm.dgu.edu/index.html">Kang Ryoung Park</a>              
              <br>
							<em>Sensors</em>, 2018
              <br>
              <a href="data/bib/sensors-v18-i06_20211201.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                We proposed lightDenseYOLO, a novel marker detector for autonomous drone landing using deep neural networks.
              </p>
            </td>
          </tr>

          <tr onmouseout="npil_stop()" onmouseover="npil_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='npil_image'>
                  <img src='images/1_markerPaper/sensors-17-01987-g004-550.png' width="160"></div>
                <img src='images/1_markerPaper/2021-12-01_16-19.png' width="160">
              </div>
              <script type="text/javascript">
                function npil_start() {
                  document.getElementById('npil_image').style.opacity = "1";
                }

                function npil_stop() {
                  document.getElementById('npil_image').style.opacity = "0";
                }
                npil_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.mdpi.com/1424-8220/17/9/1987">
                <papertitle>Remote Marker-Based Tracking for UAV Landing Using Visible-Light Camera Sensor</papertitle>
              </a>
              <br>
              <strong>Phong Nguyen</strong>
              Ki Wan Kim, 
              Young Won Lee,
              <a href="http://dm.dgu.edu/index.html">Kang Ryoung Park</a>              
              <br>
							<em>Sensors</em>, 2017
              <br>
              <a href="data/bib/sensors-v17-i09_20211201.bib">bibtex</a>
              <p></p>
              <p style="text-align:justify">
                In this research, we determined how to safely land a drone in the absence of GPS signals using our remote maker-based tracking algorithm based on the visible light camera sensor.
              </p>
            </td>
          </tr>          
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Paper Reading Group for Vietnamese </heading>
            <p>
              During my free time, I made explaining videos for exciting computer vision papers at the <a href="https://www.youtube.com/channel/UCiP4GqgAKpspuDEkYVYgbfQ">Cracking Papers 4 VN</a> Youtube channel. Here are some examples:               
            </p>
            <p style="text-align:justify">
              <iframe style="text-align:justify" width="560" height="315" src="https://www.youtube.com/embed/UN5yXZY2cS0" title="Perceiver and Perceiver IO" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> 
            </p>
            <p style="text-align:justify">
              <iframe style="text-align:justify" width="560" height="315" src="https://www.youtube.com/embed/eNDKK_AWKSk" title="Review: Heavy Rain Image Restoration" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </p>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                The credit of this website template goes to <a href="https://jonbarron.info/">Jon Barron</a>. Thank you!
                <!-- Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page. -->
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
